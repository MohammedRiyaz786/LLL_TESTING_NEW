# # import streamlit as st
# # import json
# # from datetime import datetime
# # import pandas as pd
# # from model_executor import ModelExecutor
# # from metrics_calculator import MetricsCalculator
# # from config import MODEL_CONFIGS, TASK_METRICS
# import streamlit as st
# import json
# from datetime import datetime
# import pandas as pd
# from model_executor import ModelExecutor
# from metrics_calculator import MetricsCalculator
# from config import MODEL_CONFIGS, TASK_METRICS

# def main():
#     st.title("LLM Contextual Performance Testing")
    
#     # Initialize components
#     model_executor = ModelExecutor()
#     metrics_calculator = MetricsCalculator()
    
#     # Sidebar for model and task selection
#     st.sidebar.header("Configuration")
#     task = st.sidebar.selectbox("Select Task the target model is trained for", list(TASK_METRICS.keys()))
    
    
#     # Changed: Show all big models in a single dropdown
#     big_model = st.sidebar.selectbox("Select the Tester Model", MODEL_CONFIGS["big_models"])
    
#     # HIGHLIGHT: Added classification type selection with radio buttons for Text Classification
#     classification_type = None
#     if task == "Text Classification":
#         classification_type = st.sidebar.radio("Classification Type", ["Sentiment Analysis", "Spam Detection"])
    
#     # HIGHLIGHT: Updated model selection logic to handle the new structure for Text Classification
#     if task == "Text Classification" and classification_type:
#         # Get models for the specific classification type
#         available_small_models = MODEL_CONFIGS["small_models"][task][classification_type]
#         small_model = st.sidebar.selectbox("Choose Model to be tested", available_small_models)
#     else:
#         # For other tasks, use the original logic
#         if isinstance(MODEL_CONFIGS["small_models"].get(task, ["bert-base-uncased"]), dict):
#             # Handle nested dictionary for Text Classification (fallback)
#             available_small_models = ["bert-base-uncased"]
#         else:
#             available_small_models = MODEL_CONFIGS["small_models"].get(task, ["bert-base-uncased"])
#         small_model = st.sidebar.selectbox("Choose Model to be tested", available_small_models)
    
#     # Task-specific input handling
#     kwargs = {}
#     if task == "Question Answering":
#         context = st.text_area("Context", height=150)
#         question = st.text_area("Question", height=80)
#         input_text = question
#         kwargs = {'context': context, 'question': question}
#     elif task == "Translation":
#         input_text = st.text_area("Input Text", height=150)
#         src_lang = st.text_input("Source Language Code (e.g., 'en_XX')")
#         tgt_lang = st.text_input("Target Language Code (e.g., 'fr_XX')")
#         kwargs = {'src_lang': src_lang, 'tgt_lang': tgt_lang}
#     # HIGHLIGHT: Added classification_type to kwargs for Text Classification
#     elif task == "Text Classification" and classification_type:
#         input_text = st.text_area(f"Enter text for {classification_type}", height=150)
#         kwargs = {'classification_type': classification_type}
#     else:
#         input_text = st.text_area(f"Enter test data", height=150)

#     results_dict = {
#         "timestamp": datetime.now().isoformat(),
#         "task": task,
#         "input": input_text if 'input_text' in locals() else "",
#         "big_model": {},
#         "small_model": {}
#     }
    
#     # HIGHLIGHT: Add classification_type to results_dict if applicable
#     if task == "Text Classification" and classification_type:
#         results_dict["classification_type"] = classification_type
    
#     if st.button("Evaluate your Model now"):
#         if input_text:
#             with st.spinner("Processing models..."):
#                 st.header("Results")
                
#                 # Execute Models
#                 col1, col2 = st.columns(2)
                
#                 with col1:
#                     st.subheader("Ground Truth Data")
#                     # HIGHLIGHT: Pass classification_type to execute_big_model
#                     big_model_output = model_executor.execute_big_model(big_model, input_text, task, **kwargs)
#                     st.write(big_model_output)
#                     results_dict["big_model"] = {
#                         "name": big_model,
#                         "output": big_model_output
#                     }
                    
#                 with col2:
#                     st.subheader("Data generated by targeted Model")
#                     small_model_pipeline = model_executor.get_local_model(small_model, task)
#                     if small_model_pipeline:
#                         small_model_output = model_executor.execute_small_model(
#                             small_model_pipeline, 
#                             input_text, 
#                             task, 
#                             **kwargs
#                         )
#                         st.write(small_model_output)
#                         results_dict["small_model"] = {
#                             "name": small_model,
#                             "output": small_model_output
#                         }

#                 # Calculate Metrics
#                 if big_model_output and small_model_output:
#                     st.header("Evaluation Metrics")
                    
#                     with st.spinner("Calculating metrics..."):
#                         try:
#                             # Ensure outputs are strings
#                             small_model_output = str(small_model_output)
#                             big_model_output = str(big_model_output)
                            

#                             metric_results = metrics_calculator.calculate_metrics(
#                                 prediction=small_model_output,
#                                 reference=big_model_output,
#                                 task=task
#                             )
                            
#                             # Update results dictionary
#                             results_dict["metrics"] = metric_results
#                             if metric_results:
#                                 st.subheader("Metric Scores")
#                                 metrics_df = pd.DataFrame([ 
#                                     {"Metric": metric, "Score": f"{score:.4f}"}
#                                     for metric, score in metric_results.items()
#                                 ])
#                                 st.table(metrics_df)
#                             else:
#                                 st.warning("No metrics were calculated for this task type.")
                                
#                         except Exception as e:
#                             st.error(f"Error calculating metrics: {str(e)}")

#                     # Add download button for results
#                     st.download_button(
#                         "Download Results",
#                         json.dumps(results_dict, indent=2),
#                         f"model_comparison_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
#                         "application/json"
#                     )
#         else:
#             st.warning("Please enter input text to compare models.")

# if __name__ == "__main__":
#     main()

import streamlit as st
import json
from datetime import datetime
import pandas as pd
from model_executor import ModelExecutor
from metrics_calculator import MetricsCalculator
from config import MODEL_CONFIGS, TASK_METRICS

def main():
    st.title("LLM Contextual Performance Testing")
    
    # Initialize components
    model_executor = ModelExecutor()
    metrics_calculator = MetricsCalculator()
    
    # Sidebar for model and task selection
    st.sidebar.header("Configuration")
    task = st.sidebar.selectbox("Select Task the target model is trained for", list(TASK_METRICS.keys()))
    
    
    # Changed: Show all big models in a single dropdown
    big_model = st.sidebar.selectbox("Select the Tester Model", MODEL_CONFIGS["big_models"])

    # Special handling for NER results display
    if task == "Named Entity Recognition" and isinstance(small_model_output, list):
        st.subheader("Extracted Entities:")
        
        # Create a DataFrame for cleaner display
        if small_model_output:
            entities_df = pd.DataFrame(small_model_output)
            st.dataframe(entities_df)
        else:
            st.write("No entities found")
    else:
        st.write(small_model_output)
        
    # Classification type selection with radio buttons for Text Classification
    classification_type = None
    if task == "Text Classification":
        classification_type = st.sidebar.radio("Classification Type", ["Sentiment Analysis", "Spam Detection"])
    
    # Updated model selection logic to handle the new structure for Text Classification
    if task == "Text Classification" and classification_type:
        # Get models for the specific classification type
        available_small_models = MODEL_CONFIGS["small_models"][task][classification_type]
        small_model = st.sidebar.selectbox("Choose Model to be tested", available_small_models)
    else:
        # For other tasks, use the original logic
        if isinstance(MODEL_CONFIGS["small_models"].get(task, ["bert-base-uncased"]), dict):
            # Handle nested dictionary for Text Classification (fallback)
            available_small_models = ["bert-base-uncased"]
        else:
            available_small_models = MODEL_CONFIGS["small_models"].get(task, ["bert-base-uncased"])
        small_model = st.sidebar.selectbox("Choose Model to be tested", available_small_models)
    
    # Advanced options for summarization
    show_hallucination_analysis = False
    show_ethical_analysis = False
    if task == "Summarization":
        with st.sidebar.expander("Advanced Analysis Options"):
            show_hallucination_analysis = st.checkbox("Show Hallucination Analysis", value=True)
            show_ethical_analysis = st.checkbox("Show Ethical Analysis", value=True)
    
    # Task-specific input handling
    kwargs = {}
    if task == "Question Answering":
        context = st.text_area("Context", height=150)
        question = st.text_area("Question", height=80)
        input_text = question
        kwargs = {'context': context, 'question': question}
    elif task == "Translation":
        input_text = st.text_area("Input Text", height=150)
        src_lang = st.text_input("Source Language Code (e.g., 'en_XX')")
        tgt_lang = st.text_input("Target Language Code (e.g., 'fr_XX')")
        kwargs = {'src_lang': src_lang, 'tgt_lang': tgt_lang}
    elif task == "Text Classification" and classification_type:
        input_text = st.text_area(f"Enter text for {classification_type}", height=150)
        kwargs = {'classification_type': classification_type}
    else:
        input_text = st.text_area(f"Enter test data", height=150)

    results_dict = {
        "timestamp": datetime.now().isoformat(),
        "task": task,
        "input": input_text if 'input_text' in locals() else "",
        "big_model": {},
        "small_model": {}
    }
    
    # Add classification_type to results_dict if applicable
    if task == "Text Classification" and classification_type:
        results_dict["classification_type"] = classification_type
    
    if st.button("Evaluate your Model now"):
        if input_text:
            with st.spinner("Processing models..."):
                st.header("Results")
                
                # Execute Models
                col1, col2 = st.columns(2)
                
                with col1:
                    st.subheader("Ground Truth Data")
                    big_model_output = model_executor.execute_big_model(big_model, input_text, task, **kwargs)
                    st.write(big_model_output)
                    results_dict["big_model"] = {
                        "name": big_model,
                        "output": big_model_output
                    }
                    
                with col2:
                    st.subheader("Data generated by targeted Model")
                    small_model_pipeline = model_executor.get_local_model(small_model, task)
                    if small_model_pipeline:
                        small_model_output = model_executor.execute_small_model(
                            small_model_pipeline, 
                            input_text, 
                            task, 
                            **kwargs
                        )
                        st.write(small_model_output)
                        results_dict["small_model"] = {
                            "name": small_model,
                            "output": small_model_output
                        }

                # Calculate Metrics
                if big_model_output and small_model_output:
                    st.header("Evaluation Metrics")
                    
                    with st.spinner("Calculating metrics..."):
                        try:
                            # Ensure outputs are strings
                            small_model_output = str(small_model_output)
                            big_model_output = str(big_model_output)
                            

                            metric_results = metrics_calculator.calculate_metrics(
                                prediction=small_model_output,
                                reference=big_model_output,
                                task=task
                            )
                            
                            # Update results dictionary
                            results_dict["metrics"] = metric_results
                            if metric_results:
                                st.subheader("Metric Scores")
                                metrics_df = pd.DataFrame([ 
                                    {"Metric": metric, "Score": f"{score:.4f}"}
                                    for metric, score in metric_results.items()
                                ])
                                st.table(metrics_df)
                            else:
                                st.warning("No metrics were calculated for this task type.")
                                
                        except Exception as e:
                            st.error(f"Error calculating metrics: {str(e)}")
                    
                    # HIGHLIGHT: Added hallucination analysis for summarization task
                    if task == "Summarization" and show_hallucination_analysis:
                        st.header("Hallucination Analysis")
                        
                        with st.spinner("Analyzing potential hallucinations..."):
                            try:
                                hallucination_results = metrics_calculator.analyze_hallucination(
                                    source_text=input_text,
                                    summary=small_model_output
                                )
                                
                                # Display hallucination metrics
                                st.subheader("Hallucination Metrics")
                                
                                # Display main metrics
                                hallucination_metrics = pd.DataFrame([
                                    {"Metric": "Hallucination Score", "Value": f"{hallucination_results['hallucination_score']:.4f}"},
                                    {"Metric": "Content Overlap", "Value": f"{hallucination_results['content_overlap']:.4f}"},
                                    {"Metric": "Avg. Sentence Similarity", "Value": f"{hallucination_results['avg_sentence_similarity']:.4f}"},
                                    {"Metric": "Hallucinated Words Count", "Value": hallucination_results['hallucinated_words_count']}
                                ])
                                st.table(hallucination_metrics)
                                
                                # Show potentially hallucinated elements
                                if hallucination_results['hallucinated_words']:
                                    with st.expander("Potentially Hallucinated Words"):
                                        st.write(", ".join(hallucination_results['hallucinated_words']))
                                
                                if hallucination_results['hallucinated_entities']:
                                    with st.expander("Potentially Hallucinated Entities"):
                                        st.write(", ".join(hallucination_results['hallucinated_entities']))
                                
                                if hallucination_results['potentially_hallucinated_sentences']:
                                    with st.expander("Potentially Hallucinated Sentences"):
                                        for i, sent in enumerate(hallucination_results['potentially_hallucinated_sentences'], 1):
                                            st.write(f"{i}. {sent}")
                                
                                # Add hallucination results to the export data
                                results_dict["hallucination_analysis"] = hallucination_results
                                
                            except Exception as e:
                                st.error(f"Error in hallucination analysis: {str(e)}")
                    
                    # HIGHLIGHT: Added ethical analysis for summarization task
                    if task == "Summarization" and show_ethical_analysis:
                        st.header("Ethical Analysis")
                        
                        with st.spinner("Analyzing ethical considerations..."):
                            try:
                                ethical_results = metrics_calculator.analyze_ethical_concerns(small_model_output)
                                
                                # Display ethical metrics
                                st.subheader("Ethical Considerations")
                                
                                # Overall score
                                st.metric("Ethical Concern Score", f"{ethical_results['concern_score']:.4f}")
                                
                                # Create a table for categories
                                concern_data = []
                                for category, data in ethical_results['categories'].items():
                                    concern_data.append({
                                        "Category": category.replace('_', ' ').title(),
                                        "Detected": "Yes" if data['detected'] else "No",
                                        "Matches": ", ".join(data['matches']) if data['matches'] else "None"
                                    })
                                
                                concern_df = pd.DataFrame(concern_data)
                                st.table(concern_df)
                                
                                # Add ethical results to the export data
                                results_dict["ethical_analysis"] = ethical_results
                                
                            except Exception as e:
                                st.error(f"Error in ethical analysis: {str(e)}")

                    # Add download button for results
                    st.download_button(
                        "Download Results",
                        json.dumps(results_dict, indent=2),
                        f"model_comparison_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                        "application/json"
                    )
        else:
            st.warning("Please enter input text to compare models.")

if __name__ == "__main__":
    main()